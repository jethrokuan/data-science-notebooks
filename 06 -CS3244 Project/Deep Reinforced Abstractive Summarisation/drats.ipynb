{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import nltk\n",
    "import torchtext\n",
    "from torchtext.data import Example, Field,BucketIterator, TabularDataset\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook, trange\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "\n",
    "from model import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 28.7 GB  | Proc size: 162.7 MB\n",
      "GPU RAM Free: 12206MB | Used: 0MB | Util   0% | Total 12206MB\n",
      "Mon Mar 26 14:29:53 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX TIT...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   47C    P0    58W / 250W |      0MiB / 12206MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "process = psutil.Process(os.getpid())\n",
    "print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"/diskA/jethro/cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=nltk.word_tokenize,use_vocab=True,lower=True, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(f'{PATH}/stories.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fort Hood , Texas  The shooting at Fort Hood w...</td>\n",
       "      <td>NEW : Pfc. Lance Aviles says he was ordered to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiLeaks founder Julian Assange is trying to ...</td>\n",
       "      <td>Julian Assange is seeking to avoid extradition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The news that Scotland has rejected independen...</td>\n",
       "      <td>Hammond : UK allies will breathe sigh of relie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BP plans to continue using a controversial sub...</td>\n",
       "      <td>EPA says it \"  will continue to work over the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barcelona may be licking their wounds after a ...</td>\n",
       "      <td>Barcelona 's Eric Abidal is given the all clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               story  \\\n",
       "0  Fort Hood , Texas  The shooting at Fort Hood w...   \n",
       "1  WikiLeaks founder Julian Assange is trying to ...   \n",
       "2  The news that Scotland has rejected independen...   \n",
       "3  BP plans to continue using a controversial sub...   \n",
       "4  Barcelona may be licking their wounds after a ...   \n",
       "\n",
       "                                             summary  \n",
       "0  NEW : Pfc. Lance Aviles says he was ordered to...  \n",
       "1  Julian Assange is seeking to avoid extradition...  \n",
       "2  Hammond : UK allies will breathe sigh of relie...  \n",
       "3  EPA says it \"  will continue to work over the ...  \n",
       "4  Barcelona 's Eric Abidal is given the all clea...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74063, 18516)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDataset(torchtext.data.Dataset):\n",
    "    def __init__(self, path, text_field, label_field, dfs, **kwargs):\n",
    "        fields = [(\"text\", text_field), (\"label\", label_field)]\n",
    "        examples = []\n",
    "        num_examples = dfs[path].values[:,1].shape[0]\n",
    "        for i in range(num_examples):\n",
    "            text = dfs[path][\"story\"].iloc[i]\n",
    "            label = dfs[path][\"summary\"].iloc[i]\n",
    "            examples.append(Example.fromlist([text, label], fields))\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex): return len(ex.text)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train, validation=None, test=None, **kwargs):\n",
    "        dfs = {'train': train}\n",
    "        if validation is not None:\n",
    "            dfs['validation'] = validation\n",
    "            has_validation = 'validation'\n",
    "        else:\n",
    "            has_validation = None\n",
    "        if test is not None:\n",
    "            dfs['test'] = test\n",
    "            has_test = 'test'\n",
    "        else:\n",
    "            has_test = None\n",
    "                \n",
    "        return super().splits('',\n",
    "            text_field=text_field, label_field=label_field,\n",
    "                              train='train', validation=has_validation, test=has_test,  dfs=dfs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=nltk.word_tokenize,use_vocab=True,lower=True, include_lengths=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field()\n",
    "\n",
    "pytrain, pytest = CNNDataset.splits(text_field=TEXT,\n",
    "                                    label_field=TEXT,\n",
    "                                    train=train,\n",
    "                                    test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 233358\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(pytrain, pytest, min_freq=2)\n",
    "\n",
    "tqdm.write(\"Vocabulary size: {}\".format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(pytrain, \n",
    "                            batch_size = MINI_BATCH_SIZE,\n",
    "                            device=None,\n",
    "                            sort_key=lambda x: len(x.label),\n",
    "                            sort_within_batch=True,\n",
    "                            repeat=False,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN = 100\n",
    "EMBED = 50\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(VOCAB_SIZE,EMBED,HIDDEN,bidirec=True)\n",
    "decoder = Decoder(VOCAB_SIZE,EMBED,HIDDEN*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if USE_CUDA:\n",
    "    tqdm.write(\"Using CUDA\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using %d devices\" % (torch.cuda.device_count()))\n",
    "        encoder = nn.DataParallel(encoder)\n",
    "        decoder = nn.DataParallel(decoder)\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "decoder.embedding = encoder.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss(ignore_index=TEXT.vocab.stoi['<pad>'])\n",
    "enc_optim = optim.Adam(encoder.parameters(),lr=LR)\n",
    "dec_optim = optim.Adam(decoder.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?epoch/s]\n",
      "Batches:   0%|          | 0/7407 [00:00<?, ?batch/s]\u001b[A\n",
      "\u001b[A\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-13106facdc84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_squared_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Batches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdecoding_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<s>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for epoch_idx in trange(NUM_EPOCHS, desc = \"Epochs\", unit = \"epoch\"):\n",
    "    total_loss, total_squared_loss, num_batches = 0.0, 0.0, 0\n",
    "    for batch in tqdm(train_iter, desc = \"Batches\", unit = \"batch\"):\n",
    "        inputs, lengths = batch.text\n",
    "        targets, _ = batch.label\n",
    "        decoding_start = Variable(torch.LongTensor([TEXT.vocab.stoi['<s>']]*targets.size(0))).unsqueeze(1)\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            decoding_start = decoding_start.cuda()\n",
    "\n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "        output,hidden = encoder(inputs,lengths.tolist())\n",
    "        score = decoder(decoding_start,hidden,targets.size(1),output, len(inputs))\n",
    "\n",
    "        loss = loss_function(score,targets.view(-1))\n",
    "        total_loss += loss.data[0]\n",
    "        total_squared_loss += loss.data[0]**2\n",
    "        num_batches += 1\n",
    "        loss.backward()\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "    loss_mean = total_loss / num_batches\n",
    "    loss_variance = (total_squared_loss - (total_loss**2 / num_batches)) / (num_batches - 1)\n",
    "    tqdm.write(\"loss mean: %7.4f, loss variance: %7.4f\" % (loss_mean, loss_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), \"./encoder.model\")\n",
    "torch.save(decoder.state_dict(), \"./decoder.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load(\"./encoder.model\"))\n",
    "decoder.load_state_dict(torch.load(\"./decoder.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
